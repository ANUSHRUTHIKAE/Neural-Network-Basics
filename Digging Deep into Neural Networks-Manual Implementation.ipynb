{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8b180a6",
   "metadata": {},
   "source": [
    "# Deep Learning Vs Machine Learning Vs AI\n",
    "* **AI** to mimic Human Behaviour.\n",
    "* **Machine Learning** : Statistical methods to enable prediction.\n",
    "* **Deep Learning** : To Train neural networks to learn from **large volumes** of structured and unstructured data for **complex models**\n",
    "* Deep Learning better than Machine Learning as we can train large volumes of high dimensional data to train complex models with better accuracy.\n",
    "\n",
    "# Perceptron\n",
    "* **simplest** form of a neural network. \n",
    "* It's a **single-layer** neural network with one or more inputs, a set of weights, a bias, and an output which is sent to an activation function.\n",
    "* Perceptrons, in their basic form, do not have a mechanism for updating weights.\n",
    "* The learning rule for the perceptron involves adjusting the weights based on the misclassification of training examples to reduce error.\n",
    "* Learning Rule:\n",
    "    1. Initializing Threshold and weights, bias.\n",
    "    2. Provide Input and calculate the output.\n",
    "    3. Update the weights\n",
    "    4. Repeat steps 2 and 3.\n",
    "        \n",
    "\n",
    "# Neural Network\n",
    "* It is a model inspired by the structure and functioning of the **human brain**.\n",
    "* It consists of **interconnected nodes** , or **artificial neurons**, organized in layers.\n",
    "* Information is processed and passed through these neurons, with each connection having a **weight** that determines the strength of the signal or **slope of the prediction line**.\n",
    "* **“Biases”**  are associated with each artificial neuron (node) in a layer. These bias terms are independent of the input data and are used to **shift the output** (prediction line in the axis) of the neuron.\n",
    "* Epoch – The number of times the algorithm runs on the whole training dataset.\n",
    "* Sample – A single row of a dataset.\n",
    "* Batch – It denotes the number of samples to be taken to for updating the model parameters.\n",
    "* Learning rate – It is a parameter that provides the model a scale of how much model weights should be updated.\n",
    "\n",
    "# One Layer Neural Network Architecture\n",
    "<img src=\"images/detailed[1].png\" style=\"width:600px; height:300px;\">\n",
    "\n",
    "* **Activation Function** :\n",
    "    * It is applied to the **weighted sum of inputs to the neuron (including the bias term)**, and the result becomes the output of the neuron that is then passed to the next layer\n",
    "    * It is used to introduce **non-linearity** into the model to learn **complex patterns** and make it capable of approximating any arbitrary function.\n",
    "    * used to **activate a neuron** based on a **threshold value** .\n",
    "* **Vectorizing the neural network** : \n",
    "<img src=\"images/vector[1].png\" style=\"width:300px; height:200px;\">\n",
    "\n",
    "# Two Layer Neural Network Architecture\n",
    "<img src=\"images/detailed[2].png\" style=\"width:300px; height:400px;\">\n",
    "\n",
    "* **Formula expansion** :\n",
    "<img src=\"images/formula[2].png\" style=\"width:500px; height:250px;\">\n",
    "\n",
    "* **Vectorizing the neural network** :\n",
    "<img src=\"images/vector[2][1].jpeg\" style=\"width:500px; height:500px;\">\n",
    "<img src=\"images/vector[2][2].jpeg\" style=\"width:500px; height:250px;\">\n",
    "\n",
    "# M-Samples\n",
    "<img src= \"images/m samples.jpeg\" style=\"width:500px; height:500px;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13171ffd",
   "metadata": {},
   "source": [
    "# Basic implementation\n",
    "\n",
    "## single layer single neuron architecture\n",
    "\n",
    "<img src= \"images/simple[1].jpeg\" style=\"width:300px; height:200px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57db3730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n"
     ]
    }
   ],
   "source": [
    "inputs = [1.0, 2.0, 3.0]\n",
    "weights = [2.0, 4.0, 6.0]\n",
    "bias = 3.0\n",
    "\n",
    "output = inputs[0]*weights[0] + inputs[1]*weights[1] + inputs[2]*weights[2] + bias\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f4a433",
   "metadata": {},
   "source": [
    "## single layer multiple neuron architecture\n",
    "<img src= \"images/simple[2].jpeg\" style=\"width:500px; height:500px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "560c195f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.3, 3.4, 4.5]\n"
     ]
    }
   ],
   "source": [
    "inputs = [1.0, 2.0, 3.0, 4.0]\n",
    "\n",
    "weights1 = [0.01, 0.02, 0.03, 0.04]\n",
    "weights2 = [0.02, 0.03, 0.04, 0.05]\n",
    "weights3 = [0.03, 0.04, 0.05, 0.06]\n",
    "\n",
    "bias1 = 2.0\n",
    "bias2 = 3.0\n",
    "bias3 = 4.0\n",
    "\n",
    "output = [inputs[0]*weights1[0] + inputs[1]*weights1[1] + inputs[2]*weights1[2] + inputs[3]*weights1[3] + bias1,\n",
    "          inputs[0]*weights2[0] + inputs[1]*weights2[1] + inputs[2]*weights2[2] + inputs[3]*weights2[3] + bias2,\n",
    "          inputs[0]*weights3[0] + inputs[1]*weights3[1] + inputs[2]*weights3[2] + inputs[3]*weights3[3] + bias3]\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570ae044",
   "metadata": {},
   "source": [
    "## Dot Product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9bdde3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "inputs = [1.0, 2.0, 3.0]\n",
    "weights = [2.0, 4.0, 6.0]\n",
    "bias = 3.0\n",
    "\n",
    "output = np.dot(inputs, weights) + bias\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd80c36a",
   "metadata": {},
   "source": [
    "<img src= \"images/dotproduct.jpeg\" style=\"width:500px; height:500px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee7abeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [1.0, 2.0, 3.0, 4.0]\n",
    "\n",
    "weights = [[0.01, 0.02, 0.03, 0.04] ,[0.02, 0.03, 0.04, 0.05] , [0.03, 0.04, 0.05, 0.06]]\n",
    "\n",
    "bias = [2.0 , 3.0, 4.0]\n",
    "\n",
    "output = np.dot(weights, inputs) + bias\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c84e4f",
   "metadata": {},
   "source": [
    "## Multiple Samples\n",
    "<img src= \"images/msample-vector.jpeg\" style=\"width:500px; height:500px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3e44764f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.3  3.4  4.5 ]\n",
      " [2.4  3.54 4.68]\n",
      " [2.5  3.68 4.86]]\n"
     ]
    }
   ],
   "source": [
    "# the input is a matrix of multiple samples\n",
    "\n",
    "inputs = [[1.0, 2.0, 3.0, 4.0], [2.0,3.0,4.0,5.0],[3.0,4.0,5.0,6.0]]\n",
    "\n",
    "weights = [[0.01, 0.02, 0.03, 0.04] ,[0.02, 0.03, 0.04, 0.05] , [0.03, 0.04, 0.05, 0.06]]\n",
    "\n",
    "bias = [2.0 , 3.0 , 4.0]\n",
    "\n",
    "output = np.dot(inputs,np.array(weights).T) + bias\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdcc766",
   "metadata": {},
   "source": [
    "## Multiple layers\n",
    "<img src= \"images/2layer.jpeg\" style=\"width:500px; height:500px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c4004705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.3  3.4  4.5 ]\n",
      " [2.4  3.54 4.68]\n",
      " [2.5  3.68 4.86]]\n",
      "[[5.736  3.284  8.339 ]\n",
      " [5.7662 3.2958 8.3528]\n",
      " [5.7964 3.3076 8.3666]]\n"
     ]
    }
   ],
   "source": [
    "inputs = [[1.0, 2.0, 3.0, 4.0], [2.0,3.0,4.0,5.0],[3.0,4.0,5.0,6.0]]\n",
    "\n",
    "weights_1 = [[0.01, 0.02, 0.03, 0.04] ,[0.02, 0.03, 0.04, 0.05] , [0.03, 0.04, 0.05, 0.06]]\n",
    "\n",
    "bias_1 = [2.0 , 3.0, 4.0]\n",
    "\n",
    "output_layer1 = np.dot(inputs,np.array(weights_1).T) + bias_1\n",
    "\n",
    "print(output_layer1)\n",
    "\n",
    "weights_2 = [[0.06, 0.07, 0.08] ,[0.04, 0.03, 0.02] , [0.01, 0.04, 0.04]]\n",
    "\n",
    "bias_2 = [5.0 , 3.0, 8.0]\n",
    "\n",
    "output_layer2 = np.dot(output_layer1,np.array(weights_2).T) + bias_2\n",
    "\n",
    "print(output_layer2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fb988f",
   "metadata": {},
   "source": [
    "## Creating object layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8d06b90c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0.]]\n",
      "[[ 0.03154683 -0.05097602 -0.02772224]\n",
      " [ 0.01055149 -0.04635657 -0.00102355]\n",
      " [-0.01044384 -0.04173711  0.02567514]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(10)\n",
    "\n",
    "inputs = [[1.0, 2.0, 3.0, 4.0], [2.0,3.0,4.0,5.0],[3.0,4.0,5.0,6.0]]\n",
    "\n",
    "\n",
    "class Layer_Dense:\n",
    "\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights = 0.10 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "\n",
    "layer1 = Layer_Dense(4,3)\n",
    "layer2 = Layer_Dense(3,3)\n",
    "print(layer1.biases)\n",
    "layer1.forward(inputs)\n",
    "# print(layer1.output)\n",
    "layer2.forward(layer1.output)\n",
    "print(layer2.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4dd642",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ea88f36b",
   "metadata": {},
   "source": [
    "## ReLU Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a77d34c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.11668402 0.        ]\n",
      " [0.1839874  0.        ]\n",
      " [0.25129077 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(10)\n",
    "\n",
    "X = [[1.0, 2.0, 3.0, 4.0], [2.0,3.0,4.0,5.0],[3.0,4.0,5.0,6.0]]\n",
    "\n",
    "\n",
    "class Layer_Dense:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights = 0.10 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "\n",
    "class Activation_ReLU:\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "\n",
    "layer1 = Layer_Dense(4,2)\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "layer1.forward(X)\n",
    "\n",
    "#print(layer1.output)\n",
    "activation1.forward(layer1.output)\n",
    "print(activation1.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3568897",
   "metadata": {},
   "source": [
    "## Soft Max Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9da6f4c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.11668402 -0.10275513]\n",
      " [ 0.1839874  -0.09321932]\n",
      " [ 0.25129077 -0.08368351]]\n",
      "[[0.11668402 0.        ]\n",
      " [0.1839874  0.        ]\n",
      " [0.25129077 0.        ]]\n",
      "[[ 5.00741406e-05 -2.03730542e-03  5.05272359e-03]\n",
      " [ 7.89569201e-05 -3.21242380e-03  7.96713609e-03]\n",
      " [ 1.07839700e-04 -4.38754218e-03  1.08815486e-02]]\n",
      "[[0.00673828 0.00672423 0.00677208]\n",
      " [0.00673848 0.00671634 0.00679184]\n",
      " [0.00673867 0.00670845 0.00681167]]\n",
      "[[0.3330081  0.33231371 0.33467819]\n",
      " [0.3328193  0.33172567 0.33545502]\n",
      " [0.33262964 0.3311377  0.33623267]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(10)\n",
    "\n",
    "X = [[1.0, 2.0, 3.0, 4.0], [2.0,3.0,4.0,5.0],[3.0,4.0,5.0,6.0]]\n",
    "\n",
    "\n",
    "class Layer_Dense:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights = 0.10 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "\n",
    "class Activation_ReLU:\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.maximum(0, inputs)\n",
    "class Activation_SoftMax:\n",
    "    def forward(self, inputs):\n",
    "        exp_values = np.exp(inputs - np.max(x, axis=-1, keepdims=True))\n",
    "        print(exp_values)\n",
    "        self.output = exp_values / np.sum(exp_values, axis=-1, keepdims=True)\n",
    "\n",
    "\n",
    "layer1 = Layer_Dense(4,2)\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "layer1.forward(X)\n",
    "print(layer1.output)\n",
    "\n",
    "activation1.forward(layer1.output)\n",
    "print(activation1.output)\n",
    "\n",
    "layer2 = Layer_Dense(2,3)\n",
    "activation2 = Activation_SoftMax()\n",
    "\n",
    "\n",
    "layer2.forward(activation1.output)\n",
    "print(layer2.output)\n",
    "\n",
    "activation2.forward(layer2.output)\n",
    "print(activation2.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecebd78",
   "metadata": {},
   "source": [
    "## Calculating Loss - Categorical Cross Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40929890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.11668402 -0.10275513]\n",
      " [ 0.1839874  -0.09321932]\n",
      " [ 0.25129077 -0.08368351]]\n",
      "[[0.11668402 0.        ]\n",
      " [0.1839874  0.        ]\n",
      " [0.25129077 0.        ]]\n",
      "[[ 5.00741406e-05 -2.03730542e-03  5.05272359e-03]\n",
      " [ 7.89569201e-05 -3.21242380e-03  7.96713609e-03]\n",
      " [ 1.07839700e-04 -4.38754218e-03  1.08815486e-02]]\n",
      "[[0.99500984 0.99293505 1.        ]\n",
      " [0.99214285 0.9888827  1.        ]\n",
      " [0.98928412 0.98484689 1.        ]]\n",
      "[[0.3330081  0.33231371 0.33467819]\n",
      " [0.3328193  0.33172567 0.33545502]\n",
      " [0.33262964 0.3311377  0.33623267]]\n",
      "1.1034479290784434\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(10)\n",
    "\n",
    "X = [[1.0, 2.0, 3.0, 4.0], [2.0,3.0,4.0,5.0],[3.0,4.0,5.0,6.0]]\n",
    "\n",
    "\n",
    "class Layer_Dense:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights = 0.10 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "\n",
    "class Activation_ReLU:\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.maximum(0, inputs)\n",
    "class Activation_SoftMax:\n",
    "    def forward(self, inputs):\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=-1, keepdims=True))\n",
    "        print(exp_values)\n",
    "        self.output = exp_values / np.sum(exp_values, axis=-1, keepdims=True)\n",
    "\n",
    "class Loss_CategoricalCrossEntropy(Loss):\n",
    "    def forward(self, predicted_prob , true_labels):\n",
    "        epsilon = 1e-15\n",
    "    #     clipped = predicted_prob\n",
    "        clipped = np.clip(predicted_prob , epsilon , 1-epsilon)\n",
    "        if len(true_labels.shape) == 1:\n",
    "            loss = np.mean(-np.log(clipped[range(len(predicted_prob)), true_labels]))\n",
    "        elif len(true_labels.shape) == 2:\n",
    "            loss = np.mean(-np.log(np.sum(clipped*true_labels , axis =1)))\n",
    "        self.output = loss\n",
    "\n",
    "layer1 = Layer_Dense(4,2)\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "layer1.forward(X)\n",
    "print(layer1.output)\n",
    "\n",
    "activation1.forward(layer1.output)\n",
    "print(activation1.output)\n",
    "\n",
    "layer2 = Layer_Dense(2,3)\n",
    "activation2 = Activation_SoftMax()\n",
    "\n",
    "\n",
    "layer2.forward(activation1.output)\n",
    "print(layer2.output)\n",
    "\n",
    "activation2.forward(layer2.output)\n",
    "print(activation2.output)\n",
    "\n",
    "# true_labels = np.array([1,1,1])\n",
    "\n",
    "true_labels = np.array([[0,1,0],[0,1,0],[0,1,0]])\n",
    "loss = Loss_CategoricalCrossEntropy()\n",
    "loss.forward(activation2.output, true_labels)\n",
    "print(loss.output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
