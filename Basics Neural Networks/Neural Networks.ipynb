{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb608448",
   "metadata": {},
   "source": [
    "# Perceptron - MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fb3b2b",
   "metadata": {},
   "source": [
    "<img src=\"images/Perceptron architecture.png\" style=\"width:500px; height:200px;\">\n",
    "\n",
    "* **Input Data** : Input data consists of the information or features you want the perceptron to make decisions about. The different features of the input are represented as x1, x2, x3,.. xm. Each piece of input data is represented as a numerical value. \n",
    "* **Weights** : Weights are coefficients assigned to each input. They represent the importance or impact of each input on the perceptron's decision. Weights can be positive or negative and are adjusted during the learning process. \n",
    "* **Weighted Summation/ Net input function** : The perceptron calculates the weighted sum of all input data. It multiplies each input by its corresponding weight and adds these products together. This step is also known as the weighted sum. \n",
    "    weighted_sum = (x₁ * w₁) + (x₂ * w₂) + (x₃ * w₃) + ... + (xᵢ * wᵢ)\n",
    "* **Activation Function** : The weighted sum is passed through an activation function, which determines the perceptron’s output. Common activation functions include:\n",
    "\n",
    "    * Step Function: Outputs 1 if the weighted sum is above a certain threshold, and 0 otherwise.\n",
    "    * Sigmoid Function: Outputs a value between 0 and 1, representing the probability of activation.\n",
    "    * Rectified Linear Unit (ReLU): Outputs the weighted sum if it’s positive, and 0 otherwise.\n",
    "* **Decision** : Based on the result of the activation function, the perceptron makes a decision or prediction. For binary classification tasks, this decision is often binary, such as 0 or 1.\n",
    "    * if weighted_sum ≥ θ, then y = 1 or else if weighted_sum < θ, then y = 0\n",
    "* **Learning** : The perceptron can learn from its mistakes through a learning algorithm using an optimizer. If the decision is incorrect, the weights are adjusted to improve future decisions. Common learning algorithms include the Perceptron Learning Rule and the Delta Rule (Widrow-Hoff learning).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d00c7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Step 1: Create a Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Step 2: Add input layer (with 4 input features) and first hidden layer\n",
    "model.add(Dense(units=8, activation='relu', input_dim=4))\n",
    "\n",
    "# Step 3: Add a second hidden layer\n",
    "model.add(Dense(units=6, activation='relu'))\n",
    "\n",
    "# Step 4: Add the output layer (with 3 output classes and softmax activation for classification)\n",
    "model.add(Dense(units=3, activation='softmax'))\n",
    "\n",
    "# Step 5: Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Step 6: Generate some example data\n",
    "X_train = np.random.rand(100, 4)  # Input features\n",
    "y_train = np.random.randint(0, 3, 100)  # Output labels (3 classes)\n",
    "y_train = to_categorical(y_train, num_classes=3)  # One-hot encode the labels\n",
    "# Step 7: Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=10)\n",
    "\n",
    "# Step 8: Save the trained model to a file\n",
    "# model.save('my_trained_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d58660",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "# model = load_model('my_trained_model.h5')  # Replace with the actual path to your saved model\n",
    "output= model.predict([[1,2,3,4]])\n",
    "\n",
    "# Get the class label with the highest probability\n",
    "predicted_class = np.argmax(output)\n",
    "\n",
    "# Print the predicted class label\n",
    "print(\"Predicted Class Label:\", predicted_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edd2455",
   "metadata": {},
   "source": [
    "# Types of Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f795287",
   "metadata": {},
   "source": [
    "* **Sigmoid function** :\n",
    "    * also known as logistic / non-linear funtion\n",
    "    * formula : **σ(x) = 1 / (1 + e^(-x))**\n",
    "    * The function maps any real number to a value between 0 and 1.\n",
    "    * Graph : \n",
    "        * The sigmoid function produces an **S- shaped curve** . It starts at zero, rises slowly from -∞ to ∞, and approaches 1 as the input becomes large (positive or negative).\n",
    "        * The derivative values are in the range **(0, 0.25)**, with a maximum value of approximately 0.25 occurring at the midpoint.\n",
    "        * This **bell-shaped curve** is typical of the sigmoid function's derivative. The derivative is highest (steepest) around the center (x=0)\n",
    "        <img src=\"images/Perceptron architecture.png\" style=\"width:500px; height:200px;\">\n",
    "    * Use Cases:\n",
    "        * **Binary Classification** : It squashes the network's raw output to a probability-like value between 0 and 1\n",
    "    * Advantages:\n",
    "        * **Smooth Gradient**: It leads to more stable convergence during training.\n",
    "        * **Output Range** : The output is bounded between 0 and 1, which is useful in the context of probabilities.\n",
    "    * Disadvantages:\n",
    "        * **Vanishing Gradients**\n",
    "        * **Output Not Centered at Zero**: It can slow down learning in some cases. It may lead to vanishing gradient descent or the outputs of hidden neurons in subsequent layers are influenced by the outputs of neurons in previous layers, which can become biased towards either 0 or 1.\n",
    "        * **Not Sparse** : They always produce some activation regardless of the input, which may not be efficient for certain tasks.\n",
    "    * Due to the vanishing gradient issue and the availability of better activation functions like ReLU and its variants, sigmoid functions are less commonly used in hidden layers of deep neural networks today\n",
    "* **Tanh Function** :\n",
    "    * squashes input values to a range between -1 and 1.\n",
    "    * "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9feecc",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
